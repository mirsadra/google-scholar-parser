{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","import pandas as pd\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36'\n","}\n","\n","url = \"https://scholar.google.com/scholar?start=0&q=tirzepatide&hl=en&as_sdt=0,5\"\n","\n","max_retries = 5\n","retries = 0\n","\n","while retries < max_retries:\n","    response = requests.get(url, headers=headers)\n","    \n","    if response.status_code == 200:\n","        doc = BeautifulSoup(response.text, 'html.parser')\n","        break\n","    elif response.status_code == 429:\n","        retries += 1\n","        print(f\"Received 429 error. Retrying in {2 ** retries} seconds...\")\n","        time.sleep(2 ** retries)  # Exponential backoff\n","    else:\n","        print(f\"Unexpected error: {response.status_code}\")\n","        break\n","\n","response = requests.get(url, headers=headers)\n","\n","doc = BeautifulSoup(response.text, 'html.parser')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_paperinfo(paper_url):\n","    response = requests.get(paper_url, headers=headers)\n","\n","    if response.status_code != 200:\n","        print(\"Status Code Error: \", response.status_code)\n","        raise Exception(\"Failed to get the webpage, dude!\")\n","\n","    # Parse the webpage\n","    paper_doc = BeautifulSoup(response.text, 'html.parser')\n","\n","    return paper_doc\n","\n","def get_tags(doc):\n","    paper_tag = doc.select(\"[data-lid]\")\n","    cite_tag = doc.select(\"[title=Cite] + a\")\n","    link_tag = doc.find_all(\"h3\", {\"class\":\"gs_rt\"})\n","    author_tag = doc.find_all(\"div\", {\"class\":\"gs_a\"})\n","\n","    return paper_tag, cite_tag, link_tag, author_tag\n","\n","def get_papertitle(paper_tag):\n","    paper_names = []\n","\n","    for tag in paper_tag:\n","        paper_names.append(tag.select('h3')[0].get_text())\n","\n","    return paper_names\n","\n","def get_link(link_tag):\n","    links = []\n","\n","    for i in range(len(link_tag)):\n","        links.append(link_tag[i].select('a')[0]['href'])\n","\n","    return links\n","\n","def get_author_year_publi_info(authors_tag):\n","    years = []\n","    publication = []\n","    authors = []\n","    for i in range(len(authors_tag)):\n","        authortag_text = (authors_tag[i].text).split()\n","        # Use re.search to find a match for one or more digits\n","        year_match = re.search(r'\\d+', authors_tag[i].text)\n","\n","        # Check if a match was found\n","        if year_match:\n","            year = int(year_match.group())\n","            years.append(year)\n","        else:\n","            # Handle cases where no year is found (e.g., append a default value or continue)\n","            continue\n","\n","        # Assuming the last element in the split text is the publication name\n","        publication.append(authortag_text[-1])\n","\n","        # Assuming the first two elements in the split text are the author's first and last names\n","        author = authortag_text[0] + ' ' + re.sub(',', '', authortag_text[1])\n","        authors.append(author)\n","\n","    return years, publication, authors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["paper_repos_dict = {\n","                    'Paper Title' : [],\n","                    'Year' : [],\n","                    'Author' : [],\n","                    'Publication' : [],\n","                    'Url of paper' : [] }\n","\n","# adding information in repository\n","def add_in_paper_repo(papername, year, author, publi, link):\n","    print(f\"papername length: {len(papername)}\")\n","    print(f\"year length: {len(year)}\")\n","    print(f\"author length: {len(author)}\")\n","    print(f\"publi length: {len(publi)}\")\n","    print(f\"link length: {len(link)}\")\n","\n","\n","    if not all(len(papername) == len(lst) for lst in [year, author, publi, link]):\n","        raise ValueError(\"All lists must have the same length\")\n","    paper_repos_dict['Paper Title'].extend(papername)\n","    paper_repos_dict['Year'].extend(year)\n","    paper_repos_dict['Author'].extend(author)\n","    paper_repos_dict['Publication'].extend(publi)\n","    paper_repos_dict['Url of paper'].extend(link)\n","\n","    return pd.DataFrame(paper_repos_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range (0,250,10):\n","\n","  # get url for the each page\n","  url = \"https://scholar.google.com/scholar?start={}&q=tirzepatide&hl=en&as_sdt=0,5\".format(i)\n","\n","  # function for the get content of each page\n","  doc = get_paperinfo(url)\n","\n","  # function for the collecting tags\n","  paper_tag,cite_tag,link_tag,author_tag = get_tags(doc)\n","\n","  # paper title from each page\n","  papername = get_papertitle(paper_tag)\n","\n","  # year , author , publication of the paper\n","  year , publication , author = get_author_year_publi_info(author_tag)\n","\n","  # url of the paper\n","  link = get_link(link_tag)\n","\n","  # add in paper repo dict\n","  final = add_in_paper_repo(papername, year, author, publication, link)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["csv = final[:300]\n","csv.to_csv('scrape.csv', index=False)\n","csv.head(5)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
